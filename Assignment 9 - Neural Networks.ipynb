{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as random\n",
    "from numpy.linalg import pinv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a fixed network to solve the XOR operation, where the total number of neurons is 3 and the number of layers is 2. Use the batch gradient descent for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    Layer contains an array of neurons\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.weights = args[0]\n",
    "        self.bias = args[1]\n",
    "        \"\"\" gd: for gradient descent \"\"\"\n",
    "        self.bp2_gd = []\n",
    "        self.bp3_gd = []\n",
    "        self.bp4_gd = []\n",
    "        self.a_gd = []\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\" \n",
    "        Calling on the layer will calculate the output a of the activation function \n",
    "        and z, the intermediate calculation that sums the weights and bias\n",
    "        \n",
    "        x: list of inputs\n",
    "        \"\"\"\n",
    "        self.z = self.basis_function(self.weights, self.bias, x)\n",
    "        self.a = self.activation_function(self.sigmoid, self.z)\n",
    "        return (self.z, self.a)\n",
    "        \n",
    "    def basis_function(self, w, b, x):\n",
    "        z = w.T.dot(x) + b\n",
    "        return z\n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def activation_function(self, fun, z):\n",
    "        return fun(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"A Network contains a list of layers, and functions to do feedforward and backpropagation\"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.layers = []\n",
    "        \n",
    "        \"\"\"\n",
    "        a list of number of neurons in each layer\n",
    "        i.e. [2, 2, 1] means that there are 2 neurons in the input layer and 2 in the first and 1 in the second layer\n",
    "        \"\"\"\n",
    "        self.neuron_nums = args[0]\n",
    "        # x: features to be trained\n",
    "        self.x = args[1]\n",
    "        # N: number of training data\n",
    "        self.N = args[2]\n",
    "        # t: result for comparison\n",
    "        self.t = args[3]\n",
    "        \"\"\" gd: for gradient descent\"\"\"\n",
    "        self.bp1_gd = []\n",
    "        \n",
    "    def initialize_layers(self, neuron_nums, N):\n",
    "        for i in range(len(neuron_nums) - 1): \n",
    "            self.initialize_layer(N, neuron_nums[i], neuron_nums[i+1])\n",
    "    \n",
    "    def initialize_layer(self, N, prev, curr, mu=0):\n",
    "        \"\"\" \n",
    "        Initializes weights and bias for current layer\n",
    "        N: number of training data\n",
    "        prev: number of neurons in previous layer\n",
    "        curr: number of neurons in current layer\n",
    "\n",
    "        mu = 0\n",
    "        sigma = 1 / sqrt(N) in order to avoid network saturation\n",
    "        \"\"\"\n",
    "        mu = 0\n",
    "        sigma = 1 / np.sqrt(N)\n",
    "\n",
    "        W = np.zeros((curr, prev))\n",
    "        b = np.zeros((curr, 1))\n",
    "        for c in range(curr):\n",
    "            b[c] = random.normal(mu, sigma)\n",
    "            for p in range(prev):\n",
    "                W[c][p] = random.normal(mu, sigma)\n",
    "\n",
    "        layer = Layer(W.T, b)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, layers, x, counter):\n",
    "        \"\"\"\n",
    "        Start off with the first layer, where the input is x and the counter is 0. \n",
    "        Then we increase the counter and move to the next layer. \n",
    "        The output from the previous layer will become the input for the next layer.\n",
    "\n",
    "        layers: list of layers in the network\n",
    "        x: input to the layer\n",
    "        counter: keep track of which layer we are in\n",
    "        \"\"\"\n",
    "        layer = layers[counter] # get the current layer we are in\n",
    "        z, a = layer(x) # calculate the value of a and z\n",
    "        \n",
    "        # checks whether we the number of z's is the same as the number of neurons in the layer\n",
    "        assert z.shape == (self.neuron_nums[counter+1], 1)\n",
    "        assert a.shape == (self.neuron_nums[counter+1], 1)\n",
    "        \n",
    "        layer.z = z # save the current value of z in the layer\n",
    "        layer.a = a # save the current value of a in the layer\n",
    "        layer.a_gd.append(a)\n",
    "        if (counter==len(layers)-1):\n",
    "            return # if we have reached the last layer, stop!\n",
    "        else:\n",
    "            self.feed_forward(layers, layer.a, counter+1) # else keep feeding the result forward\n",
    "    \n",
    "    def back_prop1(self, layers):\n",
    "        \"\"\" \n",
    "        Calculates the first error to be backpropagated (from the last layer)\n",
    "        \"\"\"\n",
    "        last_layer = layers[len(layers) - 1]\n",
    "        a = last_layer.a\n",
    "        z = last_layer.z\n",
    "        sigmoid = last_layer.sigmoid\n",
    "        self.bp1 = (a - self.t) * (sigmoid(z) * (1 - sigmoid(z)))\n",
    "        self.bp1_gd.append(self.bp1)\n",
    "        \n",
    "    def back_prop2(self, layers, counter, bp1):\n",
    "        \"\"\" \n",
    "        Calculates the error to be backpropagated between layers \n",
    "        \"\"\"\n",
    "        if (counter<0):\n",
    "            return\n",
    "        current_layer = layers[counter]\n",
    "        next_layer = layers[counter+1]\n",
    "        sigmoid = current_layer.sigmoid\n",
    "        z = current_layer.z\n",
    "        w = next_layer.weights.reshape(self.neuron_nums[counter+2], self.neuron_nums[counter+1])\n",
    "        assert w.shape == (self.neuron_nums[counter+2], self.neuron_nums[counter+1])\n",
    "        \n",
    "        current_layer.bp2 = np.multiply(w.T.dot(bp1), (np.multiply(sigmoid(z), (1 - sigmoid(z)))))\n",
    "        current_layer.bp2_gd.append(current_layer.bp2)\n",
    "        print(current_layer.bp2_gd)\n",
    "        self.back_prop2(layers, counter-1, current_layer.bp2)\n",
    "    \n",
    "    def back_prop3(self, layers, counter):\n",
    "        if (counter<0):\n",
    "            return\n",
    "        \n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0):\n",
    "            a = self.x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a\n",
    "        \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2 = self.bp1\n",
    "        else: \n",
    "            bp2 = current_layer.bp2\n",
    "        \n",
    "        current_layer.bp3 = bp2.dot(a.T)\n",
    "        current_layer.bp3_gd.append(current_layer.bp3)\n",
    "        self.back_prop3(layers, counter-1)\n",
    "        \n",
    "    def back_prop4(self, layers, counter):\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        current_layer = layers[counter]\n",
    "        if (counter == len(layers) - 1):\n",
    "            current_layer.bp4 = self.bp1\n",
    "            current_layer.bp4_gd.append(current_layer.bp4)\n",
    "        else:\n",
    "            current_layer.bp4 = current_layer.bp2\n",
    "            current_layer.bp4_gd.append(current_layer.bp4)\n",
    "        self.back_prop4(layers, counter-1)\n",
    "    \n",
    "    def gradient_descent(self, layers, counter, alpha):\n",
    "        \"\"\"\n",
    "        alpha refers to the rate of learning\n",
    "        \"\"\"\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        \n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0):\n",
    "            a = self.x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a\n",
    "            \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2 = self.bp1\n",
    "        else: \n",
    "            bp2 = current_layer.bp2\n",
    "    \n",
    "        w_old = current_layer.weights\n",
    "        b_old = current_layer.bias\n",
    "        current_layer.weights = w_old - (alpha / self.N) * np.sum(bp2_gd.dot(a.T))\n",
    "        current_layer.bias = b_old - alpha * bp2\n",
    "        \n",
    "        self.gradient_descent(layers, counter-1, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[  2.87299804e-05],\n",
      "        [  3.30236290e-03]])]\n",
      "[matrix([[-0.01225961],\n",
      "        [ 0.02762571]])]\n",
      "[matrix([[ 0.00083415],\n",
      "        [-0.00436362]])]\n",
      "[matrix([[-0.00332215],\n",
      "        [ 0.00960377]])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asmatrix([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "t = np.asarray([0, 1, 1, 0]).reshape((4, 1))\n",
    "# x = np.zeros((2, 1)) # the input\n",
    "# t = 0 # the result\n",
    "N = len(x)\n",
    "neuron_nums = [2, 2, 1]\n",
    "for i in range(N):\n",
    "    network = Network(neuron_nums, x[i].T, N, t[i])\n",
    "    network.initialize_layers(network.neuron_nums, network.N)\n",
    "    network.feed_forward(network.layers, x[i].T, 0)\n",
    "    network.back_prop1(network.layers)\n",
    "    network.back_prop2(network.layers, len(network.layers)-2, network.bp1)\n",
    "    network.back_prop3(network.layers, len(network.layers)-1)\n",
    "    network.back_prop4(network.layers, len(network.layers)-1)\n",
    "#     network.gradient_descent(network.layers, len(network.layers)-1, 20)\n",
    "#     network.feed_forward(network.layers, x[i].T, 0)\n",
    "# network.layers[1].bias\n",
    "len(network.layers[0].bp2_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0057662 ]\n",
      " [ 0.05290287]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-485-fe3a94e101e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-483-a8fb88a01d3c>\u001b[0m in \u001b[0;36mback_prop2\u001b[0;34m(self, layers, counter, bp1)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mcurrent_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbp1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;31m# This promotes 1-D vectors to row vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__rmul__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,1) and (2,1) not aligned: 1 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.neuron_nums[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02519494],\n",
       "       [ 0.05052484],\n",
       "       [-0.03420019]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = network.layers[1]\n",
    "prev_layer = network.layers[0]\n",
    "sigmoid = prev_layer.sigmoid\n",
    "z = prev_layer.z\n",
    "last_layer.weights.dot(network.bp1) * (sigmoid(z) * (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05548232938898897"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = network.layers[1]\n",
    "prev_layer = network.layers[0]\n",
    "z = prev_layer.z\n",
    "sigmoid = prev_layer.sigmoid\n",
    "\n",
    "np.sum(network.bp1.dot(last_layer.weights).T.dot(sigmoid(z).T.dot(1 - sigmoid(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61965343]\n",
      " [ 0.05668075]]\n"
     ]
    }
   ],
   "source": [
    "print(network.layers[0].z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-51b4fce59e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 input neurons from input layer, 2 current neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 input neurons from layer 1, 1 current neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-74be50bbd6f7>\u001b[0m in \u001b[0;36minitialize_layer\u001b[0;34m(N, prev, curr, mu)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "N = 4\n",
    "layer1 = initialize_layer(N, 2, 2) # 2 input neurons from input layer, 2 current neurons\n",
    "layer2 = initialize_layer(N, 2, 1) # 2 input neurons from layer 1, 1 current neuron\n",
    "\n",
    "layers = []\n",
    "layers.append(layer1)\n",
    "layers.append(layer2)\n",
    "x = np.zeros((2, 1)) # x1 = 0, x2 = 0\n",
    "\n",
    "def feed_forward(N, x, layer1, layer2):\n",
    "    z1, a1 = layer1(x)\n",
    "    z2, a2 = layer2(a1)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "z1, a1, z2, a2 = feed_forward(N)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
