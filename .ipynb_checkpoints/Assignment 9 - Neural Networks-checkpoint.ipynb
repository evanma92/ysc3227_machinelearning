{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as random\n",
    "from numpy.linalg import pinv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a fixed network to solve the XOR operation, where the total number of neurons is 3 and the number of layers is 2. Use the batch gradient descent for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    Layer contains an array of neurons\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.weights = args[0]\n",
    "        self.bias = args[1]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\" \n",
    "        Calling on the layer will calculate the output a of the activation function \n",
    "        and z, the intermediate calculation that sums the weights and bias\n",
    "        \n",
    "        x: list of inputs\n",
    "        \"\"\"\n",
    "        self.z = self.basis_function(self.weights, self.bias, x)\n",
    "        self.a = self.activation_function(self.sigmoid, self.z)\n",
    "        return (self.z, self.a)\n",
    "        \n",
    "    def basis_function(self, w, b, x):\n",
    "        z = w.T.dot(x) + b\n",
    "        return z\n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def activation_function(self, fun, z):\n",
    "        return fun(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"A Network contains a list of layers, and functions to do feedforward and backpropagation\"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.layers = []\n",
    "        \n",
    "        \"\"\"\n",
    "        a list of number of neurons in each layer\n",
    "        i.e. [2, 2, 1] means that there are 2 neurons in the input layer and 2 in the first and 1 in the second layer\n",
    "        \"\"\"\n",
    "        self.neuron_nums = args[0]\n",
    "        # x: features to be trained\n",
    "        self.x = args[1]\n",
    "        # N: number of training data\n",
    "        self.N = args[2]\n",
    "        \n",
    "    def initialize_layers(self, neuron_nums, N):\n",
    "        for i in range(len(neuron_nums) - 1): \n",
    "            self.initialize_layer(N, neuron_nums[i], neuron_nums[i+1])\n",
    "    \n",
    "    def initialize_layer(self, N, prev, curr, mu=0):\n",
    "        \"\"\" \n",
    "        Initializes weights and bias for current layer\n",
    "        N: number of training data\n",
    "        prev: number of neurons in previous layer\n",
    "        curr: number of neurons in current layer\n",
    "\n",
    "        mu = 0\n",
    "        sigma = 1 / sqrt(N) in order to avoid network saturation\n",
    "        \"\"\"\n",
    "        mu = 0\n",
    "        sigma = 1 / np.sqrt(N)\n",
    "\n",
    "        W = np.zeros((curr, prev))\n",
    "        b = np.zeros((curr, 1))\n",
    "        for c in range(curr):\n",
    "            b[c] = random.normal(mu, sigma)\n",
    "            for p in range(prev):\n",
    "                W[c][p] = random.normal(mu, sigma)\n",
    "\n",
    "        layer = Layer(W.T, b)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, layers, x, counter):\n",
    "        \"\"\"\n",
    "        Start off with the first layer, where the input is x and the counter is 0. \n",
    "        Then we increase the counter and move to the next layer. \n",
    "        The output from the previous layer will become the input for the next layer.\n",
    "\n",
    "        layers: list of layers in the network\n",
    "        x: input to the layer\n",
    "        counter: keep track of which layer we are in\n",
    "        \"\"\"\n",
    "        layer = layers[counter] # get the current layer we are in\n",
    "        z, a = layer(x) # calculate the value of a and z\n",
    "        layer.z = z # save the current value of z in the layer\n",
    "        layer.a = a # save the current value of a in the layer\n",
    "        if (counter==len(layers)-1):\n",
    "            return # if we have reached the last layer, stop!\n",
    "        else:\n",
    "            self.feed_forward(layers, layer.a, counter+1) # else keep feeding the result forward\n",
    "    \n",
    "    def back_prop1(self, layers):\n",
    "        \"\"\" \n",
    "        Calculates the first error to be backpropagated (from the last layer)\n",
    "        \"\"\"\n",
    "        last_layer = layers[len(layers) - 1]\n",
    "        a = last_layer.a\n",
    "        z = last_layer.z\n",
    "        sigmoid = last_layer.sigmoid\n",
    "        self.bp1 = (a - t) * (sigmoid(z) * (1 - sigmoid(z)))\n",
    "        \n",
    "    def back_prop2(self, layers, counter, bp1):\n",
    "        \"\"\" \n",
    "        Calculates the error to be backpropagated between layers \n",
    "        \"\"\"\n",
    "        if (counter<0):\n",
    "            return\n",
    "        current_layer = layers[counter]\n",
    "        next_layer = layers[counter+1]\n",
    "        sigmoid = current_layer.sigmoid\n",
    "        z = current_layer.z\n",
    "        w = next_layer.weights\n",
    "        current_layer.bp2 = w.dot(bp1) * (sigmoid(z) * (1 - sigmoid(z)))\n",
    "        self.back_prop2(layers, counter-1, current_layer.bp2)\n",
    "    \n",
    "    def back_prop3(self, layers, counter):\n",
    "        if (counter<0):\n",
    "            return\n",
    "        \n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0):\n",
    "            a = self.x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a\n",
    "        \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2 = self.bp1\n",
    "        else: \n",
    "            bp2 = current_layer.bp2\n",
    "        \n",
    "        current_layer.bp3 = bp2.dot(a.T)\n",
    "        self.back_prop3(layers, counter-1)\n",
    "        \n",
    "    def back_prop4(self, layers, counter):\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        current_layer = layers[counter]\n",
    "        if (counter == len(layers) - 1):\n",
    "            current_layer.bp4 = self.bp1\n",
    "        else:\n",
    "            current_layer.bp4 = current_layer.bp2\n",
    "        self.back_prop4(layers, counter-1)\n",
    "    \n",
    "    def gradient_descent(self, layers, counter, alpha):\n",
    "        \"\"\"\n",
    "        alpha refers to the rate of learning\n",
    "        \"\"\"\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        \n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0):\n",
    "            a = self.x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a\n",
    "            \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2 = self.bp1\n",
    "        else: \n",
    "            bp2 = current_layer.bp2\n",
    "    \n",
    "        w_old = current_layer.weights\n",
    "        b_old = current_layer.bias\n",
    "        current_layer.weights = w_old - alpha * (a.dot(bp2.T))\n",
    "        current_layer.bias = b_old - alpha * bp2\n",
    "        \n",
    "        self.gradient_descent(layers, counter-1, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0284159]])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.zeros((2, 1)) # the input\n",
    "t = 0 # the result\n",
    "N = 4\n",
    "neuron_nums = [2, 2, 1]\n",
    "network = Network(neuron_nums, x, N)\n",
    "network.initialize_layers(network.neuron_nums, network.N)\n",
    "network.feed_forward(network.layers, x, 0)\n",
    "network.back_prop1(network.layers)\n",
    "network.back_prop2(network.layers, len(network.layers)-2, network.bp1)\n",
    "network.back_prop3(network.layers, len(network.layers)-1)\n",
    "network.back_prop4(network.layers, len(network.layers)-1)\n",
    "network.gradient_descent(network.layers, len(network.layers)-1, 20)\n",
    "network.feed_forward(network.layers, x, 0)\n",
    "network.layers[1].a\n",
    "# network.layers[1].weights - network.layers[0].a.dot(network.layers[1].bp2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02519494],\n",
       "       [ 0.05052484],\n",
       "       [-0.03420019]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = network.layers[1]\n",
    "prev_layer = network.layers[0]\n",
    "sigmoid = prev_layer.sigmoid\n",
    "z = prev_layer.z\n",
    "last_layer.weights.dot(network.bp1) * (sigmoid(z) * (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05548232938898897"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_layer = network.layers[1]\n",
    "prev_layer = network.layers[0]\n",
    "z = prev_layer.z\n",
    "sigmoid = prev_layer.sigmoid\n",
    "\n",
    "np.sum(network.bp1.dot(last_layer.weights).T.dot(sigmoid(z).T.dot(1 - sigmoid(z))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61965343]\n",
      " [ 0.05668075]]\n"
     ]
    }
   ],
   "source": [
    "print(network.layers[0].z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-51b4fce59e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 input neurons from input layer, 2 current neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 2 input neurons from layer 1, 1 current neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-74be50bbd6f7>\u001b[0m in \u001b[0;36minitialize_layer\u001b[0;34m(N, prev, curr, mu)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "N = 4\n",
    "layer1 = initialize_layer(N, 2, 2) # 2 input neurons from input layer, 2 current neurons\n",
    "layer2 = initialize_layer(N, 2, 1) # 2 input neurons from layer 1, 1 current neuron\n",
    "\n",
    "layers = []\n",
    "layers.append(layer1)\n",
    "layers.append(layer2)\n",
    "x = np.zeros((2, 1)) # x1 = 0, x2 = 0\n",
    "\n",
    "def feed_forward(N, x, layer1, layer2):\n",
    "    z1, a1 = layer1(x)\n",
    "    z2, a2 = layer2(a1)\n",
    "    return z1, a1, z2, a2\n",
    "\n",
    "z1, a1, z2, a2 = feed_forward(N)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
