{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as random\n",
    "from numpy.linalg import pinv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a fixed network to solve the XOR operation, where the total number of neurons is 3 and the number of layers is 2. Use the batch gradient descent for the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \"\"\"\n",
    "    Layer contains an array of neurons\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.weights = args[0]\n",
    "        self.bias = args[1]\n",
    "        \"\"\" gd: for gradient descent \"\"\"\n",
    "        self.bp2_gd = []\n",
    "        self.bp3_gd = []\n",
    "        self.bp4_gd = []\n",
    "        self.a_gd = []\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\" \n",
    "        Calling on the layer will calculate the output a of the activation function \n",
    "        and z, the intermediate calculation that sums the weights and bias\n",
    "        \n",
    "        x: list of inputs\n",
    "        \"\"\"\n",
    "        self.z = self.basis_function(self.weights, self.bias, x)\n",
    "        self.a = self.activation_function(self.sigmoid, self.z)\n",
    "        return (self.z, self.a)\n",
    "        \n",
    "    def basis_function(self, w, b, x):\n",
    "        z = w.T.dot(x) + b\n",
    "        return z\n",
    "    \n",
    "    def sigmoid(self, a):\n",
    "        return 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    def activation_function(self, fun, z):\n",
    "        return fun(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \"\"\"A Network contains a list of layers, and functions to do feedforward and backpropagation\"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        self.layers = []\n",
    "        \n",
    "        \"\"\"\n",
    "        a list of number of neurons in each layer\n",
    "        i.e. [2, 2, 1] means that there are 2 neurons in the input layer and 2 in the first and 1 in the second layer\n",
    "        \"\"\"\n",
    "        self.neuron_nums = args[0]\n",
    "        # x: features to be trained\n",
    "        self.x = args[1]\n",
    "        # N: number of training data\n",
    "        self.N = len(self.x)\n",
    "        # t: result for comparison\n",
    "        self.t = args[2]\n",
    "        \"\"\" gd: for gradient descent\"\"\"\n",
    "        self.bp1_gd = []\n",
    "        self.error = []\n",
    "        self.error_epoch = []\n",
    "        \n",
    "    def initialize_layers(self, neuron_nums, N):\n",
    "        for i in range(len(neuron_nums) - 1): \n",
    "            self.initialize_layer(N, neuron_nums[i], neuron_nums[i+1])\n",
    "    \n",
    "    def initialize_layer(self, N, prev, curr, mu=0):\n",
    "        \"\"\" \n",
    "        Initializes weights and bias for current layer\n",
    "        N: number of training data\n",
    "        prev: number of neurons in previous layer\n",
    "        curr: number of neurons in current layer\n",
    "\n",
    "        mu = 0\n",
    "        sigma = 1 / sqrt(N) in order to avoid network saturation\n",
    "        \"\"\"\n",
    "        mu = 0\n",
    "        sigma = 1 / np.sqrt(N)\n",
    "\n",
    "        W = np.zeros((curr, prev))\n",
    "        b = np.zeros((curr, 1))\n",
    "        for c in range(curr):\n",
    "            b[c] = random.normal(mu, sigma)\n",
    "            for p in range(prev):\n",
    "                W[c][p] = random.normal(mu, sigma)\n",
    "\n",
    "        layer = Layer(W.T, b)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def feed_forward(self, layers, x, counter):\n",
    "        \"\"\"\n",
    "        Start off with the first layer, where the input is x and the counter is 0. \n",
    "        Then we increase the counter and move to the next layer. \n",
    "        The output from the previous layer will become the input for the next layer.\n",
    "\n",
    "        layers: list of layers in the network\n",
    "        x: input to the layer\n",
    "        counter: keep track of which layer we are in\n",
    "        \"\"\"\n",
    "        layer = layers[counter] # get the current layer we are in\n",
    "        z, a = layer(x) # calculate the value of a and z\n",
    "        \n",
    "        # checks whether we the number of z's is the same as the number of neurons in the layer\n",
    "        assert z.shape == (self.neuron_nums[counter+1], 1)\n",
    "        assert a.shape == (self.neuron_nums[counter+1], 1)\n",
    "        \n",
    "        layer.z = z # save the current value of z in the layer\n",
    "        layer.a = a # save the current value of a in the layer\n",
    "        layer.a_gd.append(a.T)\n",
    "        if (counter==len(layers)-1):\n",
    "            self.output = a\n",
    "            return # if we have reached the last layer, return the output\n",
    "        else:\n",
    "            self.feed_forward(layers, layer.a, counter+1) # else keep feeding the result forward\n",
    "    \n",
    "    def back_prop1(self, layers, t):\n",
    "        \"\"\" \n",
    "        Compute error of last layer\n",
    "        \"\"\"\n",
    "        last_layer = layers[len(layers) - 1]\n",
    "        a = last_layer.a\n",
    "        z = last_layer.z\n",
    "        t = t.reshape(a.shape)\n",
    "        sigmoid = last_layer.sigmoid\n",
    "        self.error.append(np.amax(np.multiply((a - t),(a-t))))\n",
    "        assert a.shape==t.shape\n",
    "        self.bp1 = np.multiply((a - t), np.multiply(sigmoid(z),(1 - sigmoid(z))))\n",
    "        assert self.bp1.shape == (self.neuron_nums[len(layers)], 1)\n",
    "        self.bp1_gd.append(self.bp1)\n",
    "        \n",
    "    def back_prop2(self, layers, counter, bp1):\n",
    "        \"\"\" \n",
    "        Backpropagating the error \n",
    "        \"\"\"\n",
    "        if (counter<0):\n",
    "            return\n",
    "        \n",
    "        \"\"\" \n",
    "        Setting up the parameters:\n",
    "              From the current layer: z, sigmoid\n",
    "              From the next layer: w, error \n",
    "        \"\"\"\n",
    "        current_layer = layers[counter]\n",
    "        next_layer = layers[counter+1]\n",
    "        sigmoid = current_layer.sigmoid\n",
    "        z = current_layer.z\n",
    "        w = next_layer.weights.T\n",
    "        assert w.shape == (self.neuron_nums[counter+2], self.neuron_nums[counter+1])\n",
    "        \n",
    "        \"\"\" Calculating the error for that layer \"\"\"\n",
    "        current_layer.bp2 = np.multiply(w.T.dot(bp1), (np.multiply(sigmoid(z), (1 - sigmoid(z)))))\n",
    "        current_layer.bp2_gd.append(current_layer.bp2)\n",
    "        assert current_layer.bp2.shape == (self.neuron_nums[counter+1], 1)\n",
    "        \n",
    "        \"\"\" Recurse and pass the error back \"\"\"\n",
    "        self.back_prop2(layers, counter-1, current_layer.bp2)\n",
    "    \n",
    "    \"\"\" Computing the Gradients back_prop3 and back_prop4 \"\"\"\n",
    "    \n",
    "    def back_prop3(self, layers, x, counter):\n",
    "        if (counter<0):\n",
    "            return\n",
    "        \n",
    "        \"\"\" Setting up the parameters \"\"\"\n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0): # if it is the first layer, 'a' comes from the input layer\n",
    "            a = x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a\n",
    "        \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2 = self.bp1\n",
    "        else: \n",
    "            bp2 = current_layer.bp2\n",
    "        \n",
    "        current_layer.bp3 = bp2.dot(a.T)\n",
    "        current_layer.bp3_gd.append(current_layer.bp3)\n",
    "        assert current_layer.bp3.shape == (self.neuron_nums[counter+1], self.neuron_nums[counter])\n",
    "        self.back_prop3(layers, x, counter-1)\n",
    "        \n",
    "    def back_prop4(self, layers, counter):\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        current_layer = layers[counter]\n",
    "        if (counter == len(layers) - 1):\n",
    "            current_layer.bp4 = self.bp1\n",
    "            current_layer.bp4_gd.append(current_layer.bp4)\n",
    "        else:\n",
    "            current_layer.bp4 = current_layer.bp2\n",
    "            current_layer.bp4_gd.append(current_layer.bp4)\n",
    "        self.back_prop4(layers, counter-1)\n",
    "    \n",
    "    def __call__(self, x, t):\n",
    "        \"\"\" Calling the network will ask it to predict \"\"\"\n",
    "        self.feed_forward(self.layers, x, 0)\n",
    "        print (self.output)\n",
    "        if (np.argmax(self.output)==0):\n",
    "            print(\"Prediction: 0\")\n",
    "            print(\"Ground Truth: %s\" % t)\n",
    "        else:\n",
    "            print(\"Prediction: 1\")\n",
    "            print(\"Ground Truth: %s\" % t)\n",
    "        \n",
    "    \n",
    "    def gradient_descent(self, layers, x, counter, alpha):\n",
    "        \"\"\" alpha refers to the rate of learning \"\"\"\n",
    "        if (counter < 0):\n",
    "            return\n",
    "        \n",
    "        \"\"\" Setting up the Parameters \"\"\"\n",
    "        current_layer = layers[counter]\n",
    "        if (counter==0):\n",
    "            a = x\n",
    "        else:\n",
    "            prev_layer = layers[counter-1]\n",
    "            a = prev_layer.a_gd\n",
    "            \n",
    "        if (counter == len(layers) - 1): # if it is the last layer, get bp1\n",
    "            bp2_gd = self.bp1_gd\n",
    "        else: \n",
    "            bp2_gd = current_layer.bp2_gd\n",
    "        \n",
    "        w_old = current_layer.weights\n",
    "        b_old = current_layer.bias\n",
    "        \n",
    "        \"\"\" Actual calculation for Gradient Descent \"\"\"\n",
    "        \n",
    "        for i in range(50): # the number of times we want to run the gradient descent\n",
    "            current_layer.bias = b_old - (alpha / self.N) * sum(bp2_gd)\n",
    "            bp2 = []\n",
    "            for n in range(len(bp2_gd)):\n",
    "                bp2.append(bp2_gd[n].dot(a[n]))\n",
    "            learning_weights = sum(bp2).T\n",
    "            assert w_old.shape == ((alpha / self.N) * learning_weights).shape\n",
    "            current_layer.weights = w_old - (alpha / self.N) * learning_weights\n",
    "            \n",
    "            b_old = current_layer.bias\n",
    "            w_old = current_layer.weights\n",
    "        \n",
    "        self.gradient_descent(layers, x, counter-1, alpha)\n",
    "        \n",
    "    def refresh_network(self):\n",
    "        layers = self.layers\n",
    "        for l in layers:\n",
    "            l.bp2_gd = []\n",
    "            l.bp3_gd = []\n",
    "            l.bp4_gd = []\n",
    "            l.a_gd = []\n",
    "        self.bp1_gd = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.asmatrix([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "t = np.asmatrix([[1, 0], [0, 1], [0, 1], [1, 0]])\n",
    "neuron_nums = [2, 3, 2]\n",
    "\n",
    "network = Network(neuron_nums, x, t)\n",
    "network.initialize_layers(network.neuron_nums, network.N)\n",
    "\n",
    "def train(x, t, neuron_nums):    \n",
    "    for i in range(len(x)):\n",
    "#         print(\"Training num: %s\" % i)\n",
    "        network.feed_forward(network.layers, network.x[i].T, 0)\n",
    "        network.back_prop1(network.layers, t[i])\n",
    "        network.back_prop2(network.layers, len(network.layers)-2, network.bp1)\n",
    "        network.back_prop3(network.layers, network.x[i].T, len(network.layers)-1)\n",
    "        network.back_prop4(network.layers, len(network.layers)-1)\n",
    "    network.gradient_descent(network.layers, x, len(network.layers)-1, 0.5)\n",
    "    network.error_epoch.append(np.mean(network.error))\n",
    "#     print(\"Training complete!\")\n",
    "\n",
    "for i in range(1000):\n",
    "    train(x, t, neuron_nums)\n",
    "    network.refresh_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show the graph of the training process, where the x-axis represents the epoch, and the y-axis represents the overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEVCAYAAAAb/KWvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSBZSuIuIZCGFu4hIFlK4i4hkof8PIUN17GzNTn8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111788160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle(\"Error with respect to number of Epochs\", fontsize=16)\n",
    "x = np.arange(0, 1000, 1)\n",
    "plt.plot(x, network.error_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify your network in the testing stage, by giving any possible values as the input. Show both your prediction and the groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.98664021]\n",
      " [ 0.01336422]]\n",
      "Prediction: 0\n",
      "Ground Truth: 0\n",
      "[[ 0.01292554]\n",
      " [ 0.98718594]]\n",
      "Prediction: 1\n",
      "Ground Truth: 1\n",
      "[[ 0.01487383]\n",
      " [ 0.98500539]]\n",
      "Prediction: 1\n",
      "Ground Truth: 1\n",
      "[[ 0.98263022]\n",
      " [ 0.01739121]]\n",
      "Prediction: 0\n",
      "Ground Truth: 0\n"
     ]
    }
   ],
   "source": [
    "network(np.array([0, 0]).reshape(2, 1), 0)\n",
    "network(np.array([1, 0]).reshape(2, 1), 1)\n",
    "network(np.array([0, 1]).reshape(2, 1), 1)\n",
    "network(np.array([1, 1]).reshape(2, 1), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
